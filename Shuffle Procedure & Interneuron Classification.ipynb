{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8f13e7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6050d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' magic 4u'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bcc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get stuff you need'''\n",
    "import numpy as np\n",
    "from tqdm import tnrange\n",
    "from tqdm.notebook import tqdm as tdqm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "from matplotlib import gridspec\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.ndimage.filters as filt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from pipeline import get_data as get\n",
    "from pipeline import process_spikes as ps\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.signal import find_peaks\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a07630",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define some functions for later '''\n",
    "def tuning_curve_bytrial(x, trial, Y, dt, b, smooth=True, normalize=False, occupancy=True):\n",
    "    '''\n",
    "    Params\n",
    "    ------\n",
    "    x : ndarray\n",
    "        variable of interest by observation; shape (n_obs, )\n",
    "    trial : ndarray\n",
    "        trial num for each observation; shape (n_obs, )\n",
    "    Y : ndarray\n",
    "        spikes per observation; shape (n_obs, n_cells)\n",
    "    dt : int\n",
    "        time per observation in seconds\n",
    "    b : int\n",
    "        bin size\n",
    "    smooth : bool\n",
    "        apply gaussian filter to firing rate; optional, default is True\n",
    "    normalize : bool\n",
    "        normalize the firing rate of each cell such that its max FR is 1, min is 0;\n",
    "        optional, default is False\n",
    "    occupancy : bool\n",
    "        return occupancy (dwell time in each bin); optional, default is True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    firing_rate : ndarray\n",
    "        binned firing rate for each trial for each cell; shape (n_trials, n_bins, n_cells)\n",
    "    centers : ndarray\n",
    "        center of each bin\n",
    "    occ : ndarray\n",
    "       dwell time in each bin; shape (n_bins, n_cells)\n",
    "    '''\n",
    "    edges = np.arange(0, np.max(x) + b, b)\n",
    "    centers = (edges[:-1] + edges[1:])/2\n",
    "    b_idx = np.digitize(x, edges)\n",
    "    if np.max(x) == edges[-1]:\n",
    "        b_idx[b_idx==np.max(b_idx)] = np.max(b_idx) - 1\n",
    "    unique_bdx = np.unique(b_idx)\n",
    "\n",
    "    # find FR in each bin\n",
    "    firing_rate = np.zeros((np.unique(trial).shape[0], unique_bdx.shape[0], Y.shape[1]))\n",
    "    occ = np.zeros((np.unique(trial).shape[0], unique_bdx.shape[0], Y.shape[1]))\n",
    "    for j in range(unique_bdx.shape[0]):\n",
    "        idx1 = (b_idx == unique_bdx[j])\n",
    "        for i, t in enumerate(np.unique(trial)):\n",
    "            idx = idx1 & (trial == t)\n",
    "            if np.sum(idx)==0:\n",
    "                print('warning: zero occupancy!')\n",
    "                firing_rate[i, j, :] = firing_rate[i, j-1, :]\n",
    "                occ[i, j, :] = 0\n",
    "            else:    \n",
    "                spike_ct = np.sum(Y[idx, :], axis=0)\n",
    "                occupancy = dt * np.sum(idx)\n",
    "                occ[i, j, :] = occupancy\n",
    "                firing_rate[i, j, :] = spike_ct / occupancy\n",
    "    if smooth:\n",
    "        firing_rate = gaussian_filter1d(firing_rate, 2, axis=1, mode='wrap')\n",
    "\n",
    "    if normalize:\n",
    "        for c in range(firing_rate.shape[1]):\n",
    "            firing_rate[:, :, c] = (firing_rate[:, :, c] - np.min(firing_rate[:, :, c]))/np.max(firing_rate[:, :, c] - np.min(firing_rate[:, :, c]))\n",
    "    \n",
    "    if occupancy:\n",
    "        return firing_rate, centers, occ\n",
    "    else: \n",
    "        return firing_rate, centers\n",
    "    \n",
    "def find8adjacentElements(test_list):\n",
    "    ''' \n",
    "    Params\n",
    "    ------\n",
    "    test_list : ndarray\n",
    "        1d array to be sorted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    neighbors : list\n",
    "        nested list where each element is a list of 8 adjacent elements to the element with the same \n",
    "        index in test_list, adjusting for the first and last four elements and not including \n",
    "    '''\n",
    "    \n",
    "    neighbors = []\n",
    "    for idx, ele in enumerate(test_list):\n",
    "    # Checking for all cases to append\n",
    "        if idx == 0:\n",
    "            neighbors.append(test_list[(idx+1):(idx + 9)])\n",
    "        elif idx == 1:\n",
    "            neighbors.append(np.concatenate((test_list[(idx - 1)],test_list[(idx+1):(idx + 8)]),axis=None))\n",
    "        elif idx == 2:\n",
    "            neighbors.append(np.concatenate((test_list[:idx],test_list[(idx+1):(idx + 7)]),axis=None))\n",
    "        elif idx == 3:\n",
    "            neighbors.append(np.concatenate((test_list[:idx],test_list[(idx+1):(idx + 6)]),axis=None))\n",
    "        elif idx == len(test_list) - 1:\n",
    "            neighbors.append(np.concatenate((test_list[(idx-8):idx]),axis=None))                     \n",
    "        elif idx == len(test_list) - 2:\n",
    "            neighbors.append(np.concatenate((test_list[(idx-7):idx],test_list[(idx + 1):]),axis=None))\n",
    "        elif idx == len(test_list) - 3:\n",
    "            neighbors.append(np.concatenate((test_list[(idx-6):idx],test_list[(idx + 1):]),axis=None))\n",
    "        elif idx == len(test_list) - 4:\n",
    "            neighbors.append(np.concatenate((test_list[(idx-5):idx],test_list[(idx + 1):]),axis = None))\n",
    "        else:\n",
    "            neighbors.append(np.concatenate((test_list[(idx - 4):idx],test_list[(idx+1):(idx + 5)]),axis=None))\n",
    "    return neighbors \n",
    "\n",
    "    return (phi ** np.arange(nt)) * sigma2 / (1 - phi ** 2)\n",
    "\n",
    "def autocorr(x,lags):\n",
    "    '''numpy.correlate'''\n",
    "    mean=x.mean()\n",
    "    var=np.var(x)\n",
    "    xp=x-mean\n",
    "    corr=np.correlate(xp,xp,'full')[len(x)-1:]/var/len(x)\n",
    "\n",
    "    return corr[:len(lags)]\n",
    "\n",
    "def find(x):\n",
    "    return x.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddee405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animal_ID</th>\n",
       "      <th>Task</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Probe_Control</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Sac_Date</th>\n",
       "      <th>Frozen_Hemisphere</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Age_WholeMonth</th>\n",
       "      <th>Age_ExtraDays</th>\n",
       "      <th>Age_Month</th>\n",
       "      <th>Aged_Days</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Behavior_Sessions</th>\n",
       "      <th>Neural_Sessions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3</td>\n",
       "      <td>RF</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>1/27/2021</td>\n",
       "      <td>R</td>\n",
       "      <td>4/23/2019</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21.217</td>\n",
       "      <td>645</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A4</td>\n",
       "      <td>RF</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>1/27/2021</td>\n",
       "      <td>R</td>\n",
       "      <td>4/23/2019</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>21.217</td>\n",
       "      <td>645</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A5</td>\n",
       "      <td>RF</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>4/6/2021</td>\n",
       "      <td>L</td>\n",
       "      <td>4/23/2019</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>23.487</td>\n",
       "      <td>714</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A6</td>\n",
       "      <td>RF</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>4/6/2021</td>\n",
       "      <td>R</td>\n",
       "      <td>4/23/2019</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>23.487</td>\n",
       "      <td>714</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A7</td>\n",
       "      <td>RF</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>4/19/2021</td>\n",
       "      <td>L</td>\n",
       "      <td>5/29/2019</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>22.730</td>\n",
       "      <td>691</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Animal_ID Task Cohort  Probe_Control Sex   Sac_Date Frozen_Hemisphere  \\\n",
       "0        A3   RF      A              0   F  1/27/2021                 R   \n",
       "1        A4   RF      A              0   F  1/27/2021                 R   \n",
       "2        A5   RF      A              0   F   4/6/2021                 L   \n",
       "3        A6   RF      A              0   F   4/6/2021                 R   \n",
       "4        A7   RF      A              0   F  4/19/2021                 L   \n",
       "\n",
       "         DOB  Age_WholeMonth  Age_ExtraDays  Age_Month  Aged_Days  Age_Group  \\\n",
       "0  4/23/2019              21              4     21.217        645          3   \n",
       "1  4/23/2019              21              4     21.217        645          3   \n",
       "2  4/23/2019              23             14     23.487        714          3   \n",
       "3  4/23/2019              23             14     23.487        714          3   \n",
       "4  5/29/2019              22             21     22.730        691          3   \n",
       "\n",
       "   Behavior_Sessions  Neural_Sessions  \n",
       "0                  6                6  \n",
       "1                  6                6  \n",
       "2                  6                6  \n",
       "3                  6                6  \n",
       "4                  6                6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Load in Animal Metadata '''\n",
    "\n",
    "animalmeta = pd.read_csv('C:/Users/Python/Desktop/Dryad/MouseMetadata.csv') # adjust path name\n",
    "\n",
    "# define some useful lists of animals based on metadata\n",
    "all_aged_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 3),'Animal_ID'])\n",
    "all_MA_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 2),'Animal_ID'])\n",
    "all_young_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 1),'Animal_ID'])\n",
    "\n",
    "cohorta_mice = np.array(animalmeta.loc[(animalmeta.Cohort == 'A'),'Animal_ID'])\n",
    "cohortb_mice = np.array(animalmeta.loc[(animalmeta.Cohort == 'B'),'Animal_ID'])\n",
    "cohortc_mice = np.array(animalmeta.loc[(animalmeta.Cohort == 'C'),'Animal_ID'])\n",
    "cohortd_mice = np.array(animalmeta.loc[(animalmeta.Cohort == 'D'),'Animal_ID'])\n",
    "\n",
    "RF_aged_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 3) & (animalmeta.Task == 'RF'),'Animal_ID'])\n",
    "RF_young_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 1) & (animalmeta.Task == 'RF'),'Animal_ID'])\n",
    "SM_aged_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 3) & (animalmeta.Task == 'SM'),'Animal_ID'])\n",
    "SM_MA_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 2) & (animalmeta.Task == 'SM'),'Animal_ID'])\n",
    "SM_young_mice = np.array(animalmeta.loc[(animalmeta.Age_Group == 1) & (animalmeta.Task == 'SM'),'Animal_ID'])\n",
    "\n",
    "all_female_mice = np.array(animalmeta.loc[(animalmeta.Sex == 'F'),'Animal_ID'])\n",
    "all_male_mice = np.array(animalmeta.loc[(animalmeta.Sex == 'M'), 'Animal_ID'])\n",
    "RF_female_mice = np.array(animalmeta.loc[(animalmeta.Sex == 'F') & (animalmeta.Task == 'RF'),'Animal_ID'])\n",
    "RF_male_mice = np.array(animalmeta.loc[(animalmeta.Sex == 'M') & (animalmeta.Task == 'RF'),'Animal_ID'])\n",
    "SM_female_mice = np.array(animalmeta.loc[(animalmeta.Sex == 'F') & (animalmeta.Task == 'SM'),'Animal_ID'])\n",
    "SM_male_mice = np.array(animalmeta.loc[(animalmeta.Sex == 'M') & (animalmeta.Task == 'SM'),'Animal_ID'])\n",
    "\n",
    "animalmeta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be947817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A3' 'A4' 'A5' 'A6' 'A7' 'Y2' 'Y3' 'Y4' 'A12' 'A13' 'Y11' 'Y16' 'Y17'\n",
      " 'Y18' 'Y19' 'Y9' 'A14' 'A15' 'A16' 'A17' 'A18' 'MA1F' 'MA2F' 'MA3M'\n",
      " 'MA4M' 'MA5M' 'MA6M' 'MA7M' 'MA8F' 'MA9F' 'MA10F' 'Y20' 'Y21' 'Y22' 'Y23'\n",
      " 'Y24' 'A19' 'A20' 'A22' 'A23' 'A24' 'Y25' 'Y26' 'Y27' 'Y28']\n"
     ]
    }
   ],
   "source": [
    "''' Complete List of Mice & Neural Data Sessions '''\n",
    "all_mice = np.array(animalmeta.Animal_ID)\n",
    "print(all_mice)\n",
    "\n",
    "#list of session IDs in order of mouse names\n",
    "all_sessions = ([['0122_record1','0123_record2','0124_record3','0125_record4','0126_record5','0127_record6'], #A3\n",
    "             ['0122_record1','0123_record2','0124_record3','0125_record4','0126_record5','0127_record6'], #A4\n",
    "             ['0401_record1','0401_record2b','0403_record3','0404_record4','0405_record5','0406_record6'], #A5\n",
    "             ['0401_record1','0402_record2','0403_record3','0404_record4','0405_record5','0406_record6'], #A6\n",
    "             ['0414_record1','0415_record2','0416_record3','0417_record4','0418_record5','0419_record6'], #A7\n",
    "             ['1024_record1','1025_record2','1026_record3','1027_record4','1028_record5_2'], #Y2; _6 excluded\n",
    "             ['1016_record1','1019_record3','1020_record4','1021_record5','1022_record6'], #Y3; _2 not collected\n",
    "             ['1114_record1','1115_record2','1116_record3','1117_record4','1118_record5','1119_record6'], # end cohort A\n",
    "             ['051822_record1','051922_record2','052022_record3','052122_record4','052222_record5','052322_record6'],\n",
    "             ['050522_record1','050622_record2','050722_record3','050822_record4','050922_record5','051022_record6'],\n",
    "             ['050522_record1','050622_record2','050722_record3','050822_record4','051022_record6'], #Y11_5 not collected\n",
    "             ['062222_record3','062322_record4','062522_record5'], #Y16_1 & 2 not collected, _6 neural excluded\n",
    "             ['062822_record1','062922_record2','063022_record3','070122_record4','070222_record5','070322_record6'],\n",
    "             ['062022_record1','062122_record2','062222_record3','062322_record4','062522_record5','062622_record6'],\n",
    "             ['062822_record1','062922_record2','063022_record3','070122_record4','070222_record5','070322_record6'], \n",
    "             ['051922_record2','052022_record3'], # end cohort B; Y9 051822_1 excluded; _2 and &_3 excluded from behavior\n",
    "             ['083022_record1','083122_record2','090122_record3'], \n",
    "             ['083022_record1','083122_record2','090122_record3','090222_record4','090322_record5','090422_record6'],\n",
    "             ['083022_record1','083122_record2','090122_record3','090222_record4'], #A16_5 excluded, A16_6 not collected \n",
    "             ['082322_record1','082422_record2','082522_record3','082622_record4','082722_record5','082822_record6'],\n",
    "             ['082322_record1real','082422_record2','082522_record3','082622_record4','082722_record5','082822_record6'],\n",
    "             ['102322_record1','102422_record2','102522_record3','102622_record4','102722_record5','102822_record6'],\n",
    "             ['102322_record1','102422_record2','102522_record3','102622_record4','102722_record5','102822_record6'],\n",
    "             ['102322_record1','102422_record2','102522_record3','102622_record4','102722_record5','102822_record6'],\n",
    "             ['103122_record2','110122_record3','110222_record4','110322_record5rep','110422_record6','110522_record7'],\n",
    "             ['110622_record1','110722_record2','110822_record3','110922_record4','111022_record5','111122_record6'],\n",
    "             ['103022_record1','103122_record2','110122_record3','110222_record4','110322_record5','110422_record6'],\n",
    "             ['103022_record1','103122_record2','110122_record3','110222_record4'], #MA7_5 excluded, 6 not collected\n",
    "             ['111322_record1','111422_record2','111522_record3','111622_record4','111722_record5','111822_record6'],\n",
    "             ['111322_record1','111422_record2','111522_record3','111622_record4','111722_record5','111822_record6'],\n",
    "             ['111322_record1','111422_record2','111522_record3','111622_record4','111722_record5','111822_record6'], \n",
    "             ['092522_record1','092622_record2','092722_record3','092822_record4','092922_record5','093022_record6'],\n",
    "             ['091822_record1','091922_record2','092022_record3','092122_record4','092222_record5','092322_record6'],\n",
    "             ['092522_record1','092622_record2','092722_record3','092822_record4','092922_record5','093022_record6'],\n",
    "             ['092522_record1','092622_record2','092722_record3','092822_record4','092922_record5','093022_record6'],\n",
    "             ['091822_record1','091922_record2','092022_record3','092122_record4','092222_record5','092322_record6'], #end cohortc\n",
    "             ['012723_record2','012823_record3','012923_record4','013023_record5','013123_record6','020123_record7'],\n",
    "             ['012623_record1','012723_record2','012823_record3','012923_record4','013023_record5','013123_record6'],\n",
    "             ['012923_record2','013023_record3','013123_record4','020123_record5','020223_record6','020323_record7'],\n",
    "             ['020923_record1','021023_record2','021123_record3','021223_record4','021323_record5','021423_record6'],\n",
    "             ['022623_record1','022723_record2','022823_record3','030123_record4','030223_record5','030323_record6'],\n",
    "             ['021623_record1','021723_record2','021823_record3','021923_record4','022023_record5','022123_record6'],\n",
    "             ['021623_record1','021723_record2','021823_record3','021923_record4','022023_record5','022123_record6'],\n",
    "             ['021623_record1','021723_record2','021823_record3','021923_record4','022023_record5rep','022123_record6'],\n",
    "             ['022623_record1','022723_record2','022823_record3','030123_record4','030223_record5','030323_record6'] #end cohort d \n",
    "            ]) #list of sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ecb7e",
   "metadata": {},
   "source": [
    "# Classify putative inhibitory interneurons based on waveforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c21e5",
   "metadata": {},
   "source": [
    "First, in all sessions, you will identify putative inhibitory interneurons based on dual thresholds on FR and waveform duration. This takes <5 minutes to run for all sessions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Classify putative inhibitory interneurons & plot Figure S6A'''\n",
    "\n",
    "#load all neural data sessions\n",
    "mice = all_mice\n",
    "sessions = all_sessions\n",
    "\n",
    "# Make a dict to hold data\n",
    "data = {}\n",
    "\n",
    "for session, m in zip(sessions, mice):\n",
    "    data[m] = {}\n",
    "    \n",
    "    for s in session:\n",
    "        data[m][s] = {}\n",
    "\n",
    "folder = 'C:/Users/Python/Desktop/Dryad/waveform_metrics/' # adjust to location of downloaded metrics folders\n",
    "save_folder = 'C:/Users/Python/Desktop/LocalData/intnspeedcells/' # adjust to save folder path\n",
    "load_folder = 'C:/Users/Python/Desktop/LocalData/filtered/' # adjust to data folder\n",
    "    \n",
    "intperc = [] \n",
    "\n",
    "colors = ['#9DE4E5','#97DD43']\n",
    "\n",
    "for m, session in zip(mice,sessions):\n",
    "    \n",
    "    seshcount = 1 \n",
    "    \n",
    "    for s in session:\n",
    "        d = data[m][s]   \n",
    "        rawdata_file = 'RAW_' + m + '_' + s + '.npy'\n",
    "        raw = np.load(load_folder + rawdata_file, allow_pickle=True)\n",
    "        cell_IDs = raw.item().get('cellsfil')\n",
    "        \n",
    "        # load waveform parameters of interest for good cells only, drop rows w/ NaN, standardize for kmeans clustering\n",
    "        file_name = m + '_' + str(seshcount) + '_metrics.csv'\n",
    "        metrics = pd.read_csv(folder + file_name)\n",
    "        fullgcmetrics = metrics[metrics['cluster_id'].isin(list(cell_IDs))]\n",
    "        gcmetrics = fullgcmetrics.dropna(subset = ['duration','PT_ratio'])\n",
    "        \n",
    "        duration = np.asarray(gcmetrics['duration'])\n",
    "        PTR = np.asarray(gcmetrics['PT_ratio'])\n",
    "        halfwidth = np.asarray(gcmetrics['halfwidth'])\n",
    "        recslope = np.asarray(gcmetrics['recovery_slope'])\n",
    "        firingrate = np.asarray(gcmetrics['firing_rate'])\n",
    "        \n",
    "        #examine duration threshold and plot\n",
    "        idx = np.where((duration < 0.35) | (firingrate > 40))[0]\n",
    "        print('Interneurons %: ' + str(len(idx)/len(cell_IDs)*100))\n",
    "        oppidx = np.setdiff1d(np.arange(0,len(firingrate),1), idx)\n",
    "        \n",
    "        intperc = np.append(intperc, np.round(100*len(idx)/len(cell_IDs), 2))\n",
    "        \n",
    "        #Plot Figure S6A:\n",
    "        if (m == 'A12') and (s == '051922_record2'):\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (3.75,1.25))\n",
    "            ax1.scatter(PTR[idx], duration[idx], color = colors[0], alpha = 1, s = 5, edgecolor = 'None')\n",
    "            ax1.scatter(PTR[oppidx], duration[oppidx], color = colors[1], alpha = 1, s = 5, edgecolor = 'None')\n",
    "            ax1.set_xlabel('Peak:Trough', fontsize = 9)\n",
    "            ax1.set_ylabel('Duration (ms)', fontsize = 9)\n",
    "            ax1.hlines(0.35,0,1.25, colors = 'k', linestyles = '--', linewidth = 0.75)\n",
    "            ax1.set_xlim([0,1.25])\n",
    "            ax1.set_xticks([0,0.5,1])\n",
    "            ax1.tick_params(labelsize = 8)\n",
    "\n",
    "            #plot histograms of widths w/ cluster labels applied\n",
    "            ax2.hist(halfwidth[idx], color = colors[0], histtype = 'step')\n",
    "            ax2.hist(halfwidth[oppidx], color = colors[1], histtype = 'step')\n",
    "            ax2.set_xlabel('Width (ms)', fontsize = 9)\n",
    "            ax2.set_ylabel('Cells', fontsize = 9)\n",
    "            ax2.tick_params(labelsize = 8)\n",
    "\n",
    "            #plot histograms of widths w/ cluster labels applied\n",
    "            ax3.hist(firingrate[idx], color = colors[0], histtype = 'step')\n",
    "            ax3.hist(firingrate[oppidx], color = colors[1], histtype = 'step')\n",
    "            ax3.set_xlabel('FR (Hz)', fontsize = 9)\n",
    "            ax3.tick_params(labelsize = 8)\n",
    "            ax3.set_ylim(ax2.get_ylim())\n",
    "\n",
    "            fig.suptitle(m + '_' + s[-1:] + ', IN %: ' + str(np.round(100*len(idx)/len(cell_IDs) , 2)), fontsize = 10)\n",
    "            plt.subplots_adjust(top = 0.8)\n",
    "            #plt.savefig(save_folder + m + '_' + s + '_' + 'qualityint.png', dpi=400, bbox_inches='tight')\n",
    "            #plt.savefig(save_folder + m + '_' + s + '_' + 'qualityint.svg', dpi=400, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "        seshcount += 1\n",
    "        \n",
    "        #create boolean (0 = excitatory cell; 1 = interneuron)\n",
    "        intn = np.zeros(len(cell_IDs))\n",
    "        intn[idx] = 1\n",
    "        intn[oppidx] = 0\n",
    "          \n",
    "        #save the boolean interneuron classifcation    \n",
    "        intn_file = 'intn_' + m + '_' + s + '.npy'\n",
    "        np.save(save_folder + intn_file, intn)\n",
    "        print('saved interneuron classification as npy file for mouse ' + m + ' session '+ s)\n",
    "\n",
    "print('Mean, SEM % interneurons / session:')\n",
    "print(np.mean(intperc), stats.sem(intperc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b9a6b",
   "metadata": {},
   "source": [
    "# Run shuffle & calculate scores for each rep for RF sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616688ba",
   "metadata": {},
   "source": [
    "In Random Foraging (RF) sessions, we classified speed-tuned cells and spatial cells. In half of the RF sessions (male mice), dark trial data is available to differentiate grid and non-grid spatial cells. In those sessions, we calculated the height of the shuffle's dark FR autocorrelation at the same lag as the largest peak in the real dark FR autocorrelation. Note that if you prefer not to re-run the shuffle procedure, its output is available in the Dryad repository accompanying this publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f275a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load in the RF sessions only & reset dictionary'''\n",
    "# Define mice, sessions for particular cohorts\n",
    "mice , mice_ind, _  = np.intersect1d(all_mice,(np.union1d(cohorta_mice, cohortb_mice)), return_indices = True) # RF mice\n",
    "#mice , mice_ind, _  = np.intersect1d(all_mice,np.union1d(cohortc_mice, cohortd_mice), return_indices = True) # SM mice\n",
    "\n",
    "# If you found the intersection \n",
    "sessions = []\n",
    "for i in mice_ind:\n",
    "    sessions.append(all_sessions[i])\n",
    "\n",
    "# Make a dict to hold data\n",
    "data = {}\n",
    "\n",
    "for session, m in zip(sessions, mice):\n",
    "    data[m] = {}\n",
    "    \n",
    "    for s in session:\n",
    "        data[m][s] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate 100x Shuffled Scores for each Cell in all RF Sessions \n",
    "\n",
    "Shuffle speed & speed stability scores are used to classify speed-tuning. \n",
    "Shuffle spatial sparsity & coherence scores are used to classify position-tuning, identifying spatial cells.\n",
    "Shuffle dark FR autocorrelation max peak height at the same lag as the actual dark max peak is used to differentiate\n",
    "grid from non-grid spatial cells in the mice with dark trials (Cohort B).\n",
    "\n",
    "Expected run time is hours per session.\n",
    "\n",
    "'''\n",
    "n_rep = 100 # number of shuffles\n",
    "sigma = 20  # smoothing factor for speed & speed stability scores\n",
    "n_bin = 8 #n position bins considered for speed stability score\n",
    "track_length = 400 #cm\n",
    "bins = np.arange(0,track_length + (track_length/n_bin),(track_length/n_bin))\n",
    "n_dark = 20 #trials\n",
    "lags = np.arange(0,801,1)\n",
    "\n",
    "save_folder = 'C:/Users/Python/Desktop/LocalData/shuffscores/' #adjust path \n",
    "dataload_folder = 'C:/Users/Python/Desktop/LocalData/filtered/'#adjust path to output of Import & Filter notebook\n",
    "\n",
    "for m, session in zip(mice,sessions):\n",
    "    for s in tdqm(session):\n",
    "        d = data[m][s]\n",
    "        \n",
    "        #load in data\n",
    "        rawdata_file = 'RAW_' + m + '_' + s + '.npy'\n",
    "        behavior_file = 'BEHAVIOR_' + m + '_' + s + '.npy'\n",
    "        \n",
    "        raw = np.load(dataload_folder + rawdata_file, allow_pickle=True)\n",
    "        behaviorfil = np.load(dataload_folder + behavior_file, allow_pickle=False)\n",
    "        \n",
    "        rawspeed = raw.item().get('speed')\n",
    "        speed = behaviorfil[:,1]\n",
    "        \n",
    "        sp = raw.item().get('sp')\n",
    "        spiket = sp['st'].copy()\n",
    "        cluster_id = sp['clu'].copy()\n",
    "        cell_IDs = raw.item().get('cellsfil')\n",
    "        posx = raw.item().get('posx')\n",
    "        post = raw.item().get('post')\n",
    "        \n",
    "        dt = np.unique(np.round(np.diff(post),4))\n",
    "        if len(dt) > 1: # discard duplicate frame entries if they occurred\n",
    "            dt = dt[dt != 0]\n",
    "            dt_to_trash = np.where(np.diff(post) == 0)[0]\n",
    "        else:\n",
    "            dt_to_trash = [] \n",
    "            \n",
    "        posxfil = behaviorfil[:,0]\n",
    "        trial = behaviorfil[:,2]\n",
    "        trials = np.unique(trial)\n",
    "        postfil = behaviorfil[:,3]\n",
    "        \n",
    "        # filter spikes & position frames by speed < 2cm/s or out of track limits \n",
    "        speed_to_trash = find(rawspeed < 2)\n",
    "        pos_to_trash = find((posx < 0) | (posx > 400))\n",
    "        trash_idx = np.unique(np.concatenate((dt_to_trash, speed_to_trash, pos_to_trash)))\n",
    "        keep_idx = np.setdiff1d(np.arange(len(rawspeed)), trash_idx)\n",
    "        \n",
    "        # find indices of spike corresponding to 8 x 50cm bins along the track based on filtered position\n",
    "        bin_idx = []\n",
    "        for i in range(n_bin):\n",
    "            if i == (n_bin - 1):\n",
    "                idx = np.where((posxfil >= bins[i]) & (posxfil <= bins[i + 1]))[0]\n",
    "            else:\n",
    "                idx = np.where((posxfil >= bins[i]) & (posxfil < bins[i + 1]))[0]\n",
    "            bin_idx.append(idx)\n",
    "            \n",
    "        if m in cohortb_mice:\n",
    "            spikes_file = 'SPIKES_' + m + '_' + s +'.npy'\n",
    "            Y = np.load(dataload_folder + spikes_file, allow_pickle=False) #pre-filtered for speeds >2cm/s, position-corrected spikes\n",
    "            \n",
    "            #get dark-only, normalized, smoothed spatial map & reshape into continuous vector for each cell\n",
    "            FR, _ , _ = ps.tuning_curve_bytrial(posxfil, trial, Y, dt, b = 2, smooth=True, normalize=True, occupancy=True)\n",
    "            stop_idx = np.where(trials == n_dark)[0][0]\n",
    "            FR = FR[:stop_idx,:,:]\n",
    "            n_trials = FR.shape[0]\n",
    "            n_pos = FR.shape[1]\n",
    "            n_cells = FR.shape[2]\n",
    "            FR = (np.reshape(FR, (n_trials*n_pos, n_cells))).T\n",
    "\n",
    "            #find max autocorrelation peaks for all cells; store height & prom of most prominent peak\n",
    "            peak = [] # boolean if peak found\n",
    "            maxpeak_locs = [] #stored lag of max peak if peak exist\n",
    "            maxpeak_heights = [] # stored height of max peak, otherwise 0 if not exist\n",
    "            maxpeak_proms = [] # stored prominence of max peak, otherwise 0 if not exist\n",
    "            for i, c in enumerate(cell_IDs):\n",
    "                sdx = np.where(cell_IDs == c)[0][0]\n",
    "                autocorrelation = autocorr(FR[sdx,:],lags)\n",
    "                normauto = autocorrelation / autocorrelation[0]\n",
    "                peaks, properties = find_peaks(normauto, width = 10, height = 0.10, prominence = 0.15)\n",
    "                proms = properties[\"prominences\"]\n",
    "                heights = properties[\"peak_heights\"]\n",
    "\n",
    "                if peaks.size > 0:\n",
    "                    peak = np.append(peak,1)\n",
    "                    maxpeak_locs = np.append(maxpeak_locs, peaks[np.argmax(proms)])\n",
    "                    maxpeak_heights = np.append(maxpeak_heights, heights[np.argmax(proms)])\n",
    "                    maxpeak_proms = np.append(maxpeak_proms, proms[np.argmax(proms)])\n",
    "                else:\n",
    "                    peak = np.append(peak,0)\n",
    "                    maxpeak_heights = np.append(maxpeak_heights,0)\n",
    "                    maxpeak_proms = np.append(maxpeak_proms,0)\n",
    "\n",
    "            peak = peak.astype(bool)\n",
    "            \n",
    "            shuffmaxpeak_heights = np.zeros((n_rep, len(cell_IDs[peak])))\n",
    "            \n",
    "        #calculate & save shuffled speed scores\n",
    "        shuffspeedscore = np.zeros((n_rep, len(cell_IDs)))\n",
    "        shuffspeedstabscore = np.zeros((n_rep, len(cell_IDs)))\n",
    "        shuffspatialcoherencescores = np.zeros((n_rep,len(cell_IDs)))\n",
    "        shuffsparsityscores = np.zeros((n_rep,len(cell_IDs)))\n",
    "        \n",
    "        for n in tdqm(range(0, n_rep)):\n",
    "            \n",
    "            shuffspeed_score = []\n",
    "            shuffspeedstab_score = []\n",
    "            shuffspatialsparseness = []\n",
    "            shuffspatialcoherence = []\n",
    "            shuffheights = []\n",
    "            \n",
    "            B = np.zeros((len(rawspeed), len(cell_IDs))) # set empty shuffled FR matrix shape n_obs, n_cells\n",
    "            for i, c in enumerate(cell_IDs):\n",
    "                sdx = (np.where(cell_IDs==c)[0][0]).astype(int)\n",
    "                \n",
    "                #get actual spike times for each cell\n",
    "                st = spiket[cluster_id == c]\n",
    "                st = st[(st >= min(post)) & (st <= max(post))]\n",
    "\n",
    "                #get shuffle times\n",
    "                st_shuf = st - min(post)\n",
    "                total_time = max(post) - min(post)\n",
    "                add = np.random.uniform(0.02,total_time,1)\n",
    "                st_shuf = (st_shuf + add) % total_time\n",
    "                st_shuf = st_shuf + min(post)\n",
    "                                                        \n",
    "                #get unfiltered spike train\n",
    "                spike_ct = np.zeros_like(post)\n",
    "                spike_idx = np.digitize(st_shuf,post) #obs corresponding to where each shuffled spike occurs\n",
    "\n",
    "                idx, cts = np.unique(spike_idx, return_counts = True)\n",
    "                spike_ct[idx] = cts\n",
    "                B[:,i] = spike_ct\n",
    "\n",
    "                #check for & interpolate any missing values; smooth if necessary\n",
    "                if sum(np.isnan(B[:,i])) > 0:\n",
    "                    B[:,i] = get.nan_interp(B[:,i])  \n",
    "\n",
    "            #apply filter spike train by speed & posx errors\n",
    "            B = B[keep_idx,:]\n",
    "            total_time = max(postfil) - min(postfil)\n",
    "            \n",
    "            #convert spike train / instantaneous FR into smoothed & unsmoothed FR vectors\n",
    "            smoothFR, _ , smoothocc = ps.tuning_curve(posxfil, B, dt, b = 2, l=2, smooth=True, SEM=False, occupancy=True)\n",
    "            FR, _ , _ = ps.tuning_curve(posxfil, B, dt, b = 2, l=2, smooth=False, SEM=False, occupancy=True)\n",
    "\n",
    "            for i, c in enumerate(cell_IDs):\n",
    "\n",
    "                #calculate & store speed score for this cell's smoothed spike train\n",
    "                B[:,i] = gaussian_filter1d(B[:,i],sigma)\n",
    "                sscore, _ = stats.pearsonr(B[:,i],speed)\n",
    "                shuffspeed_score = np.append(shuffspeed_score,sscore)\n",
    "                \n",
    "                #calculate & store speed stability score for this cell's smoothed spike train\n",
    "                bin_pspikes = []\n",
    "                for j in range(n_bin):\n",
    "                    idx = bin_idx[j]\n",
    "                    psscore, _ = stats.pearsonr(B[idx,i],speed[idx])\n",
    "                    bin_pspikes = np.append(bin_pspikes, (psscore *  np.sum(B[idx,i])))\n",
    "                \n",
    "                sstabscore = np.sum(bin_pspikes) / np.sum(B[:,i])\n",
    "                shuffspeedstab_score = np.append(shuffspeedstab_score,sstabscore)\n",
    "                \n",
    "                #calculate spatial sparsity score with smoothed FR \n",
    "                meanFRsqr = np.square(np.mean(smoothFR[:,i]))\n",
    "                products = []\n",
    "                for b in range(smoothFR.shape[0]):\n",
    "                    binmeanFRsqr = np.square(smoothFR[b,i])\n",
    "                    prob = smoothocc[b,i]/total_time\n",
    "                    prod = prob * binmeanFRsqr\n",
    "                    products = np.append(products,prod)\n",
    "                sparsity = np.sum(products) / meanFRsqr\n",
    "                shuffspatialsparseness = np.append(shuffspatialsparseness,sparsity)  \n",
    "                \n",
    "                # calculate spatial tuning coherence score with non-smoothed FR\n",
    "                neighbors = find8adjacentElements(FR[:,i])\n",
    "                meanFR_neighbors = []\n",
    "                for b in range(FR.shape[0]):\n",
    "                    meanFR_neigh = np.mean(neighbors[b])\n",
    "                    meanFR_neighbors = np.append(meanFR_neighbors, meanFR_neigh)\n",
    "                coherence, _ = stats.pearsonr(FR[:,i],meanFR_neighbors)\n",
    "                if math.isnan(coherence):\n",
    "                    shuffspatialcoherence = np.append(shuffspatialcoherence,0)\n",
    "                else:\n",
    "                    shuffspatialcoherence = np.append(shuffspatialcoherence,coherence)\n",
    "                    \n",
    "            shuffspeedscore[n,:] = shuffspeed_score #forms array (n_shuffreps, n_goodcells)\n",
    "            shuffspeedstabscore[n,:] = shuffspeedstab_score #forms array (n_shuffreps, n_goodcells)\n",
    "            shuffsparsityscores[n,:] = shuffspatialsparseness\n",
    "            shuffspatialcoherencescores[n,:] = shuffspatialcoherence\n",
    "            \n",
    "            # for mice with dark trials...\n",
    "            if (m in cohortb_mice):         \n",
    "                \n",
    "                FR, _ , _ = ps.tuning_curve_bytrial(posxfil, trial, B, dt, b = 2, smooth=True, normalize=True, occupancy=True)\n",
    "                FR = FR[:stop_idx,:,:]\n",
    "                n_trials = FR.shape[0]\n",
    "                n_pos = FR.shape[1]\n",
    "                n_cells = FR.shape[2]\n",
    "                FR = (np.reshape(FR, (n_trials*n_pos, n_cells))).T\n",
    "                \n",
    "                for i, c in enumerate(cell_IDs[peak]):\n",
    "                    sdx = (np.where(cell_IDs==c)[0][0]).astype(int)\n",
    "                    autocorrelation = autocorr(FR[sdx,:],lags)\n",
    "                    normauto = autocorrelation / autocorrelation[0]\n",
    "                    preflag_height = normauto[maxpeak_locs[i].astype(int)]\n",
    "                    shuffheights = np.append(shuffheights, preflag_height)\n",
    "\n",
    "                shuffmaxpeak_heights[n,:] = shuffheights\n",
    "            \n",
    "        #save scores for all cells in a session in nested lists    \n",
    "        d['shuffspeedscore'] = shuffspeedscore\n",
    "        d['shuffspeedstabscore'] = shuffspeedstabscore\n",
    "        d['shuffspatialcoherencescores'] = shuffspatialcoherencescores\n",
    "        d['shuffsparsityscores'] = shuffsparsityscores\n",
    "                \n",
    "        #Get file names & save shuffled scores as npy files\n",
    "        shufinstspeedscore_file = 'shufinstspeedscore_' + m + '_' + s + '.npy'\n",
    "        shufspeedstabscore_file = 'shufspeedstabscore_' + m + '_' + s + '.npy'\n",
    "        shufsparsityscore_file = 'shufsparsityscores_' + m + '_' + s + '.npy'\n",
    "        shufspatialcoherencescore_file = 'shufspatialcoherencescores_' + m + '_' + s + '.npy'\n",
    "        \n",
    "        np.save(save_folder + shufinstspeedscore_file, d['shuffspeedscore'])\n",
    "        print('saved shuffled speed score data as npy file for mouse ' + m + ' session '+ s)\n",
    "        \n",
    "        np.save(save_folder + shufspeedstabscore_file, d['shuffspeedstabscore'])\n",
    "        print('saved shuffled speed stability score data as npy file for mouse ' + m + ' session '+ s)\n",
    "\n",
    "        np.save(save_folder + shufsparsityscore_file, d['shuffsparsityscores'])\n",
    "        print('saved shuffled sparsity scores as npy file for mouse ' + m + ' session '+ s)\n",
    "\n",
    "        np.save(save_folder + shufspatialcoherencescore_file, d['shuffspatialcoherencescores'])\n",
    "        print('saved shuffled spatial coherence scores as npy file for mouse ' + m + ' session '+ s)\n",
    "        \n",
    "        if m in cohortb_mice:\n",
    "            #save scores for all cells in a session in nested lists    \n",
    "            d['shuffmaxpeak_heights'] = shuffmaxpeak_heights\n",
    "\n",
    "            #Get file names & save shuffled scores, grid cell boolean for spatial cells as npy files\n",
    "            shufmaxpeak_heights_file = 'shufmaxpeak_heights__' + m + '_' + s + '.npy'\n",
    "\n",
    "            np.save(save_folder + shufmaxpeak_heights_file, d['shuffmaxpeak_heights'])\n",
    "            print('saved shuffled max peak heights data as npy file for mouse ' + m + ' session '+ s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2016134",
   "metadata": {},
   "source": [
    "# Run shuffle & calculate scores for each rep for SM sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e21e8",
   "metadata": {},
   "source": [
    "For Split Maze sessions as for RF sessions, we will classify speed-tuned cells and spatial cells. Given the multiple contexts in the SM task and spatial maps for each context, I calculated the spatial tuning curve for Block A (trials 20-80) only and sparsity and coherence scores for that period only to classify spatial cells. For SM sessions, we also calculate the peakiness of the dark FR autocorrelation on each shuffle to differentiate grid from non-grid spatial cells as above. Note that if you prefer not to re-run the shuffle procedure, its output is available in the Dryad repository accompanying this publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8699f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load in the SM sessions only'''\n",
    "# Define mice, sessions for particular cohorts\n",
    "#mice , mice_ind, _  = np.intersect1d(all_mice,(np.union1d(cohorta_mice, cohortb_mice)), return_indices = True) # RF mice\n",
    "mice , mice_ind, _  = np.intersect1d(all_mice,np.union1d(cohortc_mice, cohortd_mice), return_indices = True) # SM mice\n",
    "\n",
    "# If you found the intersection \n",
    "sessions = []\n",
    "for i in mice_ind:\n",
    "    sessions.append(all_sessions[i])\n",
    "\n",
    "# Make a dict to hold data\n",
    "data = {}\n",
    "\n",
    "for session, m in zip(sessions, mice):\n",
    "    data[m] = {}\n",
    "    \n",
    "    for s in session:\n",
    "        data[m][s] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72469063",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate 100x Shuffled Scores for each Cell in all SM Sessions \n",
    "\n",
    "Shuffle speed & speed stability scores are used to classify speed-tuning. \n",
    "Shuffle spatial sparsity & coherence scores are used to classify position-tuning, identifying spatial cells.\n",
    "Shuffle dark FR autocorrelation max peak height at the same lag as the actual dark max peak is used to differentiate\n",
    "grid from non-grid spatial cells. \n",
    "\n",
    "Expected run time is hours per session.\n",
    "\n",
    "'''\n",
    "n_rep = 100 # number of shuffles\n",
    "sigma = 20  # smoothing factor for speed & speed stability scores\n",
    "n_bin = 8 #n position bins considered for speed stability score\n",
    "track_length = 400 #cm\n",
    "bins = np.arange(0,track_length + (track_length/n_bin),(track_length/n_bin))\n",
    "n_dark = 20 #trials\n",
    "lags = np.arange(0,801,1)\n",
    "\n",
    "save_folder = 'C:/Users/Python/Desktop/LocalData/shuffscores/' #adjust path \n",
    "dataload_folder = 'C:/Users/Python/Desktop/LocalData/filtered/'#adjust path to output of Import & Filter notebook\n",
    "\n",
    "for m, session in zip(mice,sessions):\n",
    "    for s in tdqm(session):\n",
    "        d = data[m][s]\n",
    "        \n",
    "        #load in data\n",
    "        rawdata_file = 'RAW_' + m + '_' + s + '.npy'\n",
    "        behavior_file = 'BEHAVIOR_' + m + '_' + s + '.npy'\n",
    "        spikes_file = 'SPIKES_' + m + '_' + s +'.npy'\n",
    "            \n",
    "        raw = np.load(dataload_folder + rawdata_file, allow_pickle=True)\n",
    "        behaviorfil = np.load(dataload_folder + behavior_file, allow_pickle=False)\n",
    "        Y = np.load(dataload_folder + spikes_file, allow_pickle=False) #pre-filtered for speeds >2cm/s, position-corrected spikes\n",
    "        \n",
    "        rawspeed = raw.item().get('speed')\n",
    "        speed = behaviorfil[:,1]\n",
    "        \n",
    "        sp = raw.item().get('sp')\n",
    "        spiket = sp['st'].copy()\n",
    "        cluster_id = sp['clu'].copy()\n",
    "        cell_IDs = raw.item().get('cellsfil')\n",
    "        posx = raw.item().get('posx')\n",
    "        post = raw.item().get('post')\n",
    "        \n",
    "        dt = np.unique(np.round(np.diff(post),4))\n",
    "        if len(dt) > 1: # discard duplicate frame entries if they occurred\n",
    "            dt = dt[dt != 0]\n",
    "            dt_to_trash = np.where(np.diff(post) == 0)[0]\n",
    "        else:\n",
    "            dt_to_trash = [] \n",
    "            \n",
    "        posxfil = behaviorfil[:,0]\n",
    "        trial = behaviorfil[:,2]\n",
    "        trials = np.unique(trial)\n",
    "        postfil = behaviorfil[:,3]\n",
    "        \n",
    "        # filter spikes & position frames by speed < 2cm/s or out of track limits \n",
    "        speed_to_trash = find(rawspeed < 2)\n",
    "        pos_to_trash = find((posx < 0) | (posx > 400))\n",
    "        trash_idx = np.unique(np.concatenate((dt_to_trash, speed_to_trash, pos_to_trash)))\n",
    "        keep_idx = np.setdiff1d(np.arange(len(rawspeed)), trash_idx)\n",
    "        \n",
    "        #get dark-only, normalized, smoothed spatial map & reshape into continuous vector for each cell\n",
    "        FR, _ , _ = ps.tuning_curve_bytrial(posxfil, trial, Y, dt, b = 2, smooth=True, normalize=True, occupancy=True)\n",
    "        stop_idx = np.where(trials == n_dark)[0][0]\n",
    "        FR = FR[:stop_idx,:,:]\n",
    "        n_trials = FR.shape[0]\n",
    "        n_pos = FR.shape[1]\n",
    "        n_cells = FR.shape[2]\n",
    "        FR = (np.reshape(FR, (n_trials*n_pos, n_cells))).T\n",
    "        print(FR.shape)\n",
    "        \n",
    "        #find max autocorrelation peaks for all cells; store height & prom of most prominent peak\n",
    "        peak = [] # boolean if peak found\n",
    "        maxpeak_locs = [] #stored lag of max peak if peak exist\n",
    "        maxpeak_heights = [] # stored height of max peak, otherwise 0 if not exist\n",
    "        maxpeak_proms = [] # stored prominence of max peak, otherwise 0 if not exist\n",
    "        for i, c in enumerate(cell_IDs):\n",
    "            sdx = np.where(cell_IDs == c)[0][0]\n",
    "            autocorrelation = autocorr(FR[sdx,:],lags)\n",
    "            normauto = autocorrelation / autocorrelation[0]\n",
    "            peaks, properties = find_peaks(normauto, width = 10, height = 0.10, prominence = 0.15)\n",
    "            proms = properties[\"prominences\"]\n",
    "            heights = properties[\"peak_heights\"]\n",
    "\n",
    "            if peaks.size > 0:\n",
    "                peak = np.append(peak,1)\n",
    "                maxpeak_locs = np.append(maxpeak_locs, peaks[np.argmax(proms)])\n",
    "                maxpeak_heights = np.append(maxpeak_heights, heights[np.argmax(proms)])\n",
    "                maxpeak_proms = np.append(maxpeak_proms, proms[np.argmax(proms)])\n",
    "            else:\n",
    "                peak = np.append(peak,0)\n",
    "                maxpeak_heights = np.append(maxpeak_heights,0)\n",
    "                maxpeak_proms = np.append(maxpeak_proms,0)\n",
    "                \n",
    "        peak = peak.astype(bool)\n",
    "        \n",
    "        # find indices of spike corresponding to 8 x 50cm bins along the track based on filtered position\n",
    "        bin_idx = []\n",
    "        for i in range(n_bin):\n",
    "            if i == (n_bin - 1):\n",
    "                idx = np.where((posxfil >= bins[i]) & (posxfil <= bins[i + 1]))[0]\n",
    "            else:\n",
    "                idx = np.where((posxfil >= bins[i]) & (posxfil < bins[i + 1]))[0]\n",
    "            bin_idx.append(idx)\n",
    "        \n",
    "        #find start and end of the Block A trials\n",
    "        start_idx = (np.where(trial == 20)[0][0]).astype(int) # trial is zero-indexed, 20 full dark trials\n",
    "        end_idx = (np.where(trial == 79)[0][-1]).astype(int) # get indices of all 60 context_a trials that follow dark\n",
    "            \n",
    "        #calculate & save shuffled speed scores\n",
    "        shuffspeedscore = np.zeros((n_rep, len(cell_IDs)))\n",
    "        shuffspeedstabscore = np.zeros((n_rep, len(cell_IDs)))\n",
    "        shuffspatialacoherencescores = np.zeros((n_rep,len(cell_IDs)))\n",
    "        shuffsparsityascores = np.zeros((n_rep,len(cell_IDs)))\n",
    "        shuffmaxpeak_heights = np.zeros((n_rep, len(cell_IDs[peak])))\n",
    "\n",
    "        for n in tdqm(range(0, n_rep)):\n",
    "            \n",
    "            shuffspeed_score = []\n",
    "            shuffspeedstab_score = []\n",
    "            shuffspatialasparseness = []\n",
    "            shuffspatialacoherence = []\n",
    "            shuffheights = []\n",
    "            \n",
    "            B = np.zeros((len(rawspeed), len(cell_IDs))) # reset empty shuffled FR matrix\n",
    "            \n",
    "            for i, c in enumerate(cell_IDs):\n",
    "                sdx = (np.where(cell_IDs==c)[0][0]).astype(int)\n",
    "                \n",
    "                #get actual spike times for each cell\n",
    "                st = spiket[cluster_id == c]\n",
    "                st = st[(st >= min(post)) & (st <= max(post))]\n",
    "\n",
    "                #get shuffle times\n",
    "                st_shuf = st - min(post)\n",
    "                total_time = max(post) - min(post)\n",
    "                add = np.random.uniform(0.02,total_time,1)\n",
    "                st_shuf = (st_shuf + add) % total_time\n",
    "                st_shuf = st_shuf + min(post)\n",
    "                                                        \n",
    "                #get unfiltered spike train\n",
    "                spike_ct = np.zeros_like(post)\n",
    "                spike_idx = np.digitize(st_shuf,post) #obs corresponding to where each shuffled spike occurs\n",
    "\n",
    "                idx, cts = np.unique(spike_idx, return_counts = True)\n",
    "                spike_ct[idx] = cts\n",
    "                B[:,i] = spike_ct\n",
    "\n",
    "                #check for & interpolate any missing values; smooth if necessary\n",
    "                if sum(np.isnan(B[:,i])) > 0:\n",
    "                    B[:,i] = get.nan_interp(B[:,i])  \n",
    "\n",
    "            #apply filter spike train by speed & posx errors\n",
    "            B = B[keep_idx,:]\n",
    "            \n",
    "            #select the Block A spike train\n",
    "            atotal_time = max(postfil[start_idx:end_idx]) - min(postfil[start_idx:end_idx])\n",
    "            A = B[start_idx:end_idx, :]\n",
    "            aposxfil = posxfil[start_idx:end_idx]\n",
    "            \n",
    "            #convert spike train / instantaneous FR into smoothed & unsmoothed FR vectors\n",
    "            asmoothFR, _, asmoothocc = ps.tuning_curve(aposxfil, A, dt, b = 2, l=2, smooth=True, SEM=False, occupancy=True)\n",
    "            aFR, _ , _ = ps.tuning_curve(aposxfil, A, dt, b = 2, l = 2, smooth=False, SEM=False, occupancy=True)\n",
    "            \n",
    "            #calculate dark FR \n",
    "            FR, _ , _ = ps.tuning_curve_bytrial(posxfil, trial, B, dt, b = 2, smooth=True, normalize=True, occupancy=True)\n",
    "            stop_idx = np.where(trials == n_dark)[0][0]\n",
    "            FR = FR[:stop_idx,:,:]\n",
    "            n_trials = FR.shape[0]\n",
    "            n_pos = FR.shape[1]\n",
    "            n_cells = FR.shape[2]\n",
    "            FR = (np.reshape(FR, (n_trials*n_pos, n_cells))).T\n",
    "            print(FR.shape)\n",
    "\n",
    "            for i, c in enumerate(cell_IDs):\n",
    "\n",
    "                #calculate & store speed score for this cell's smoothed spike train\n",
    "                B[:,i] = gaussian_filter1d(B[:,i],sigma)\n",
    "                sscore, _ = stats.pearsonr(B[:,i],speed)\n",
    "                shuffspeed_score = np.append(shuffspeed_score,sscore)\n",
    "                \n",
    "                #calculate & store speed stability score for this cell's smoothed spike train\n",
    "                bin_pspikes = []\n",
    "                for j in range(n_bin):\n",
    "                    idx = bin_idx[j]\n",
    "                    psscore, _ = stats.pearsonr(B[idx,i],speed[idx])\n",
    "                    bin_pspikes = np.append(bin_pspikes, (psscore *  np.sum(B[idx,i])))\n",
    "                \n",
    "                sstabscore = np.sum(bin_pspikes) / np.sum(B[:,i])\n",
    "                shuffspeedstab_score = np.append(shuffspeedstab_score,sstabscore)\n",
    "                \n",
    "                #calculate sparsity with smoothed FR in Context A\n",
    "                meanFRsqr = np.square(np.mean(asmoothFR[:,i]))\n",
    "                products = []\n",
    "                for b in range(asmoothFR.shape[0]):\n",
    "                    binmeanFRsqr = np.square(asmoothFR[b,i])\n",
    "                    prob = asmoothocc[b,i]/atotal_time\n",
    "                    prod = prob * binmeanFRsqr\n",
    "                    products = np.append(products,prod)\n",
    "                asparsity = np.sum(products) / meanFRsqr\n",
    "                    \n",
    "                shuffspatialasparseness = np.append(shuffspatialasparseness,asparsity)  \n",
    "\n",
    "                # calculate spatial tuning coherence score with non-smoothed FR\n",
    "                neighbors = find8adjacentElements(aFR[:,i])\n",
    "                meanFR_neighbors = []\n",
    "                for b in range(aFR.shape[0]):\n",
    "                    meanFR_neigh = np.mean(neighbors[b])\n",
    "                    meanFR_neighbors = np.append(meanFR_neighbors, meanFR_neigh)\n",
    "                acoherence, _ = stats.pearsonr(aFR[:,i],meanFR_neighbors)\n",
    "                if math.isnan(acoherence):\n",
    "                    shuffspatialacoherence = np.append(shuffspatialacoherence,0)\n",
    "                else:\n",
    "                    shuffspatialacoherence = np.append(shuffspatialacoherence,acoherence)\n",
    "                  \n",
    "                # if cell had a peak in its dark FR autocorrelation, find shuffle autocorrelation height at that same lag\n",
    "                if c in cell_IDs[peak]:\n",
    "                    cdx = (np.where(cell_IDs[peak] == c)[0][0]).astype(int)\n",
    "                    sdx = np.where(cell_IDs == c)[0][0]\n",
    "                    autocorrelation = autocorr(FR[sdx,:],lags)\n",
    "                    normauto = autocorrelation / autocorrelation[0]\n",
    "                    preflag_height = normauto[maxpeak_locs[cdx].astype(int)]\n",
    "                    shuffheights = np.append(shuffheights, preflag_height)\n",
    "                    \n",
    "            shuffspeedscore[n,:] = shuffspeed_score #forms array (n_shuffreps, n_goodcells)\n",
    "            shuffspeedstabscore[n,:] = shuffspeedstab_score #forms array (n_shuffreps, n_goodcells)\n",
    "            shuffsparsityascores[n,:] = shuffspatialasparseness #forms array (n_shuffreps, n_goodcells)\n",
    "            shuffspatialacoherencescores[n,:] = shuffspatialacoherence #forms array (n_shuffreps, n_goodcells)\n",
    "            shuffmaxpeak_heights[n,:] = shuffheights\n",
    "            \n",
    "        #save scores for all cells in a session in nested lists    \n",
    "        d['shuffspeedscore'] = shuffspeedscore\n",
    "        d['shuffspeedstabscore'] = shuffspeedstabscore\n",
    "        d['shuffspatialacoherencescores'] = shuffspatialacoherencescores\n",
    "        d['shuffsparsityascores'] = shuffsparsityascores\n",
    "        d['shuffmaxpeak_heights'] = shuffmaxpeak_heights\n",
    "                           \n",
    "        #Get file names & save shuffled scores as npy files\n",
    "        shufinstspeedscore_file = 'shufinstspeedscore_' + m + '_' + s + '.npy'\n",
    "        shufspeedstabscore_file = 'shufspeedstabscore_' + m + '_' + s + '.npy'\n",
    "        shufsparsityascore_file = 'shufsparsityascores_' + m + '_' + s + '.npy'\n",
    "        shufspatialacoherencescore_file = 'shufspatialacoherencescores_' + m + '_' + s + '.npy'\n",
    "        shufmaxpeak_heights_file = 'shufmaxpeak_heights__' + m + '_' + s + '.npy'\n",
    "        \n",
    "        np.save(save_folder + shufinstspeedscore_file, d['shuffspeedscore'])\n",
    "        print('saved shuffled speed score data as npy file for mouse ' + m + ' session '+ s)\n",
    "        \n",
    "        np.save(save_folder + shufspeedstabscore_file, d['shuffspeedstabscore'])\n",
    "        print('saved shuffled speed stability score data as npy file for mouse ' + m + ' session '+ s)\n",
    "\n",
    "        np.save(save_folder + shufsparsityascore_file, d['shuffsparsityascores'])\n",
    "        print('saved shuffled Context A sparsity scores as npy file for mouse ' + m + ' session '+ s)\n",
    "\n",
    "        np.save(save_folder + shufspatialacoherencescore_file, d['shuffspatialacoherencescores'])\n",
    "        print('saved shuffled Context A spatial coherence scores as npy file for mouse ' + m + ' session '+ s)\n",
    "        \n",
    "        np.save(save_folder + shufmaxpeak_heights_file, d['shuffmaxpeak_heights'])\n",
    "        print('saved shuffled max peak heights data as npy file for mouse ' + m + ' session '+ s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
